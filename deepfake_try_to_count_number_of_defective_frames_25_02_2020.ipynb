{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deepfake_try_to_count_number_of_defective_frames_25_02_2020.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "adf3dc7090274f65914983ebf937b2b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_60bcda75a70f4a70b133a4df3ddcae88",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_6aa5d25dfc4f4e33ad13c6570d34a791",
              "IPY_MODEL_c45a789a8cf744d09c837c72a72b858e"
            ]
          }
        },
        "60bcda75a70f4a70b133a4df3ddcae88": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6aa5d25dfc4f4e33ad13c6570d34a791": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ab9a1ab909a0404881ac1455a7df3dcb",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 200,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 200,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_540ab093c4044e8ca6c361e60183d12b"
          }
        },
        "c45a789a8cf744d09c837c72a72b858e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c633f7ac1192498b97d2cac2aee40041",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 200/200 [04:23&lt;00:00,  1.32s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7b20b18154bd4545b51fce171930948e"
          }
        },
        "ab9a1ab909a0404881ac1455a7df3dcb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "540ab093c4044e8ca6c361e60183d12b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c633f7ac1192498b97d2cac2aee40041": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7b20b18154bd4545b51fce171930948e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLK_qizQI_tI",
        "colab_type": "code",
        "outputId": "7dbcd3b8-78a9-4c07-bb8e-1ad2e220bdea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qm_SOPwCJJua",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "#import matplotlib.pylab as plt\n",
        "import cv2\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import log_loss\n",
        "XCEPTION_MODEL = '/content/gdrive/My\\ Drive/deepfakemodelspackages/xception-b5690688.pth'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ducRWRT1BG5P",
        "colab_type": "code",
        "outputId": "233404eb-1478-497a-d7c1-dc193576a49a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd /content/gdrive/My\\ Drive/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlC8S7MZJchn",
        "colab_type": "code",
        "outputId": "8ed8308a-316f-4ca4-e63b-d497a8b04167",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%time\n",
        "# Install packages\n",
        "!pip install /content/gdrive/My\\ Drive/deepfakemodelspackages/Pillow-6.2.1-cp36-cp36m-manylinux1_x86_64.whl -f ./ --no-index\n",
        "!pip install /content/gdrive/My\\ Drive/deepfakemodelspackages/munch-2.5.0-py2.py3-none-any.whl -f ./ --no-index\n",
        "!pip install /content/gdrive/My\\ Drive/deepfakemodelspackages/numpy-1.17.4-cp36-cp36m-manylinux1_x86_64.whl -f ./ --no-index\n",
        "!pip install /content/gdrive/My\\ Drive/deepfakemodelspackages/pretrainedmodels-0.7.4/pretrainedmodels-0.7.4/ -f ./ --no-index\n",
        "!pip install /content/gdrive/My\\ Drive/deepfakemodelspackages/six-1.13.0-py2.py3-none-any.whl -f ./ --no-index\n",
        "!pip install /content/gdrive/My\\ Drive/deepfakemodelspackages/torchvision-0.4.2-cp36-cp36m-manylinux1_x86_64.whl -f ./ --no-index\n",
        "!pip install /content/gdrive/My\\ Drive/deepfakemodelspackages/tqdm-4.40.2-py2.py3-none-any.whl -f ./ --no-index"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: ./\n",
            "Processing ./deepfakemodelspackages/Pillow-6.2.1-cp36-cp36m-manylinux1_x86_64.whl\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: Pillow\n",
            "  Found existing installation: Pillow 7.0.0\n",
            "    Uninstalling Pillow-7.0.0:\n",
            "      Successfully uninstalled Pillow-7.0.0\n",
            "Successfully installed Pillow-6.2.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Looking in links: ./\n",
            "Processing ./deepfakemodelspackages/munch-2.5.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from munch==2.5.0) (1.12.0)\n",
            "Installing collected packages: munch\n",
            "Successfully installed munch-2.5.0\n",
            "Looking in links: ./\n",
            "Processing ./deepfakemodelspackages/numpy-1.17.4-cp36-cp36m-manylinux1_x86_64.whl\n",
            "\u001b[31mERROR: tensorflow-model-optimization 0.2.1 requires enum34~=1.1, which is not installed.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-federated 0.12.0 has requirement tensorflow~=2.1.0, but you'll have tensorflow 1.15.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-federated 0.12.0 has requirement tensorflow-addons~=0.7.0, but you'll have tensorflow-addons 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: numpy\n",
            "  Found existing installation: numpy 1.18.1\n",
            "    Uninstalling numpy-1.18.1:\n",
            "      Successfully uninstalled numpy-1.18.1\n",
            "Successfully installed numpy-1.17.4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Looking in links: ./\n",
            "Processing ./deepfakemodelspackages/pretrainedmodels-0.7.4/pretrainedmodels-0.7.4\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from pretrainedmodels==0.7.4) (1.4.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from pretrainedmodels==0.7.4) (0.5.0)\n",
            "Requirement already satisfied: munch in /usr/local/lib/python3.6/dist-packages (from pretrainedmodels==0.7.4) (2.5.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pretrainedmodels==0.7.4) (4.28.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision->pretrainedmodels==0.7.4) (1.17.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision->pretrainedmodels==0.7.4) (1.12.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision->pretrainedmodels==0.7.4) (6.2.1)\n",
            "Building wheels for collected packages: pretrainedmodels\n",
            "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-cp36-none-any.whl size=60962 sha256=733b04dbfe657aa8c3192a04b9eeafa7ead18a4f227e4c0b740006bd133f51a6\n",
            "  Stored in directory: /root/.cache/pip/wheels/84/57/72/e804ef9827059ac7b673ba9cc1e9c5515e5ac8af8ed1e90b04\n",
            "Successfully built pretrainedmodels\n",
            "Installing collected packages: pretrainedmodels\n",
            "Successfully installed pretrainedmodels-0.7.4\n",
            "Looking in links: ./\n",
            "Processing ./deepfakemodelspackages/six-1.13.0-py2.py3-none-any.whl\n",
            "\u001b[31mERROR: tensorflow-model-optimization 0.2.1 requires enum34~=1.1, which is not installed.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-federated 0.12.0 has requirement tensorflow~=2.1.0, but you'll have tensorflow 1.15.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-federated 0.12.0 has requirement tensorflow-addons~=0.7.0, but you'll have tensorflow-addons 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement six~=1.12.0, but you'll have six 1.13.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: six\n",
            "  Found existing installation: six 1.12.0\n",
            "    Uninstalling six-1.12.0:\n",
            "      Successfully uninstalled six-1.12.0\n",
            "Successfully installed six-1.13.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "six"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Looking in links: ./\n",
            "Processing ./deepfakemodelspackages/torchvision-0.4.2-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision==0.4.2) (1.17.4)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.3.1 (from torchvision==0.4.2) (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for torch==1.3.1 (from torchvision==0.4.2)\u001b[0m\n",
            "Looking in links: ./\n",
            "Processing ./deepfakemodelspackages/tqdm-4.40.2-py2.py3-none-any.whl\n",
            "Installing collected packages: tqdm\n",
            "  Found existing installation: tqdm 4.28.1\n",
            "    Uninstalling tqdm-4.28.1:\n",
            "      Successfully uninstalled tqdm-4.28.1\n",
            "Successfully installed tqdm-4.40.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tqdm"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 432 ms, sys: 82.3 ms, total: 514 ms\n",
            "Wall time: 1min 12s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Po22BYPTLbEI",
        "colab_type": "code",
        "outputId": "e0dbcdf6-c28a-4c1f-ad90-e515eb86cf66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pip install dlib"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: dlib in /usr/local/lib/python3.6/dist-packages (19.18.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eTiy47aN6rt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## detect_from_video.py\n",
        "\"\"\"\n",
        "Evaluates a folder of video files or a single file with a xception binary\n",
        "classification network.\n",
        "Usage:\n",
        "python detect_from_video.py\n",
        "    -i <folder with video files or path to video file>\n",
        "    -m <path to model file>\n",
        "    -o <path to output folder, will write one or multiple output videos there>\n",
        "Author: Andreas RÃ¶ssler\n",
        "\"\"\"\n",
        "import os\n",
        "import argparse\n",
        "from os.path import join\n",
        "import cv2\n",
        "import dlib\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from PIL import Image as pil_image\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# from network.models import model_selection\n",
        "# from dataset.transform import xception_default_data_transforms\n",
        "\n",
        "def get_boundingbox(face, width, height, scale=1.3, minsize=None):\n",
        "    \"\"\"\n",
        "    Expects a dlib face to generate a quadratic bounding box.\n",
        "    :param face: dlib face class\n",
        "    :param width: frame width\n",
        "    :param height: frame height\n",
        "    :param scale: bounding box size multiplier to get a bigger face region\n",
        "    :param minsize: set minimum bounding box size\n",
        "    :return: x, y, bounding_box_size in opencv form\n",
        "    \"\"\"\n",
        "    x1 = face.left()\n",
        "    y1 = face.top()\n",
        "    x2 = face.right()\n",
        "    y2 = face.bottom()\n",
        "    size_bb = int(max(x2 - x1, y2 - y1) * scale)\n",
        "    if minsize:\n",
        "        if size_bb < minsize:\n",
        "            size_bb = minsize\n",
        "    center_x, center_y = (x1 + x2) // 2, (y1 + y2) // 2\n",
        "\n",
        "    # Check for out of bounds, x-y top left corner\n",
        "    x1 = max(int(center_x - size_bb // 2), 0)\n",
        "    y1 = max(int(center_y - size_bb // 2), 0)\n",
        "    # Check for too big bb size for given x, y\n",
        "    size_bb = min(width - x1, size_bb)\n",
        "    size_bb = min(height - y1, size_bb)\n",
        "\n",
        "    return x1, y1, size_bb\n",
        "\n",
        "\n",
        "def preprocess_image(image, cuda=True):\n",
        "    \"\"\"\n",
        "    Preprocesses the image such that it can be fed into our network.\n",
        "    During this process we envoke PIL to cast it into a PIL image.\n",
        "    :param image: numpy image in opencv form (i.e., BGR and of shape\n",
        "    :return: pytorch tensor of shape [1, 3, image_size, image_size], not\n",
        "    necessarily casted to cuda\n",
        "    \"\"\"\n",
        "    # Revert from BGR\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    # Preprocess using the preprocessing function used during training and\n",
        "    # casting it to PIL image\n",
        "    preprocess = xception_default_data_transforms['test']\n",
        "    preprocessed_image = preprocess(pil_image.fromarray(image))\n",
        "    # Add first dimension as the network expects a batch\n",
        "    preprocessed_image = preprocessed_image.unsqueeze(0)\n",
        "    if cuda:\n",
        "        preprocessed_image = preprocessed_image.cuda()\n",
        "    return preprocessed_image\n",
        "\n",
        "\n",
        "def predict_with_model(image, model, post_function=nn.Softmax(dim=1),\n",
        "                       cuda=True):\n",
        "    \"\"\"\n",
        "    Predicts the label of an input image. Preprocesses the input image and\n",
        "    casts it to cuda if required\n",
        "    :param image: numpy image\n",
        "    :param model: torch model with linear layer at the end\n",
        "    :param post_function: e.g., softmax\n",
        "    :param cuda: enables cuda, must be the same parameter as the model\n",
        "    :return: prediction (1 = fake, 0 = real)\n",
        "    \"\"\"\n",
        "    # Preprocess\n",
        "    preprocessed_image = preprocess_image(image, cuda)\n",
        "\n",
        "    # Model prediction\n",
        "    output = model(preprocessed_image)\n",
        "    output = post_function(output)\n",
        "\n",
        "    # Cast to desired\n",
        "    _, prediction = torch.max(output, 1)    # argmax\n",
        "    prediction = float(prediction.cpu().numpy())\n",
        "\n",
        "    return int(prediction), output\n",
        "\n",
        "def test_full_image_network(video_path, model, output_path,\n",
        "                            start_frame=0, end_frame=None, cuda=True):\n",
        "    \"\"\"\n",
        "    Reads a video and evaluates a subset of frames with the a detection network\n",
        "    that takes in a full frame. Outputs are only given if a face is present\n",
        "    and the face is highlighted using dlib.\n",
        "    :param video_path: path to video file\n",
        "    :param model_path: path to model file (should expect the full sized image)\n",
        "    :param output_path: path where the output video is stored\n",
        "    :param start_frame: first frame to evaluate\n",
        "    :param end_frame: last frame to evaluate\n",
        "    :param cuda: enable cuda\n",
        "    :return:\n",
        "    \n",
        "    # Modified to take in the model file instead of model\n",
        "    \"\"\"\n",
        "    #print('Starting: {}'.format(video_path))\n",
        "\n",
        "    # Read and write\n",
        "    p=[]\n",
        "    reader = cv2.VideoCapture(video_path)\n",
        "\n",
        "    video_fn = video_path.split('/')[-1].split('.')[0]+'.avi'\n",
        "    os.makedirs(output_path, exist_ok=True)\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n",
        "    fps = reader.get(cv2.CAP_PROP_FPS)\n",
        "    num_frames = int(reader.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    writer = None\n",
        "\n",
        "    # Face detector\n",
        "    face_detector = dlib.get_frontal_face_detector()\n",
        "\n",
        "    # Load model\n",
        "#     model, *_ = model_selection(modelname='xception', num_out_classes=2)\n",
        "#     if model_path is not None:\n",
        "#         model = torch.load(model_path)\n",
        "#         print('Model found in {}'.format(model_path))\n",
        "#     else:\n",
        "#         print('No model found, initializing random model.')\n",
        "#     if cuda:\n",
        "#         model = model.cuda()\n",
        "\n",
        "    # Text variables\n",
        "    font_face = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    thickness = 2\n",
        "    font_scale = 1\n",
        "    fake_count=0\n",
        "    # Frame numbers and length of output video\n",
        "    frame_num = 0\n",
        "    assert start_frame < num_frames - 1\n",
        "    #end_frame= num_frames\n",
        "    \n",
        "    end_frame = end_frame if end_frame else num_frames\n",
        "    pbar = tqdm(total=end_frame-start_frame)\n",
        "\n",
        "    while reader.isOpened():\n",
        "        _, image = reader.read()\n",
        "        if image is None:\n",
        "            break\n",
        "        frame_num += 1\n",
        "\n",
        "        if frame_num < start_frame:\n",
        "            continue\n",
        "        pbar.update(1)\n",
        "\n",
        "        # Image size\n",
        "#         print('getting image size')\n",
        "        height, width = image.shape[:2]\n",
        "\n",
        "        # Init output writer\n",
        "#         print('init output writer')\n",
        "        if writer is None:\n",
        "            writer = cv2.VideoWriter(join(output_path, video_fn), fourcc, fps,\n",
        "                                     (height, width)[::-1])\n",
        "\n",
        "        # 2. Detect with dlib\n",
        "#         print('detect with dlib')\n",
        "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "        faces = face_detector(gray, 1)\n",
        "        if len(faces):\n",
        "            # For now only take biggest face\n",
        "            face = faces[0]\n",
        "\n",
        "            # --- Prediction ---------------------------------------------------\n",
        "            # Face crop with dlib and bounding box scale enlargement\n",
        "            x, y, size = get_boundingbox(face, width, height)\n",
        "            cropped_face = image[y:y+size, x:x+size]\n",
        "\n",
        "            # Actual prediction using our model\n",
        "            prediction, output = predict_with_model(cropped_face, model,\n",
        "                                                    cuda=cuda)\n",
        "            # ------------------------------------------------------------------\n",
        "            \n",
        "            # Text and bb\n",
        "            x = face.left()\n",
        "            y = face.top()\n",
        "            w = face.right() - x\n",
        "            h = face.bottom() - y\n",
        "            if(prediction == 1):\n",
        "                label = 'fake' \n",
        "                fake_count=fake_count+1\n",
        "            else :\n",
        "                label='real'\n",
        "            color = (0, 255, 0) if prediction == 0 else (0, 0, 255)\n",
        "            output_list = ['{0:.2f}'.format(float(x)) for x in\n",
        "                           output.detach().cpu().numpy()[0]]\n",
        "            cv2.putText(image, str(output_list)+'=>'+label, (x, y+h+30),\n",
        "                        font_face, font_scale,\n",
        "                        color, thickness, 2)\n",
        "            # draw box over face\n",
        "            cv2.rectangle(image, (x, y), (x + w, y + h), color, 2)\n",
        "\n",
        "        if frame_num >= end_frame:\n",
        "            break\n",
        "\n",
        "        # Show\n",
        "#         print('show result')\n",
        "        # cv2.imshow('test', image)\n",
        "#         cv2.waitKey(33)     # About 30 fps\n",
        "        writer.write(image)\n",
        "    pbar.close()\n",
        "    if writer is not None:\n",
        "        writer.release()\n",
        "        #print('Finished! Output saved under {}'.format(output_path))\n",
        "    else:\n",
        "        pass\n",
        "        #print('Input video file was empty')\n",
        "    p.append(num_frames)\n",
        "    p.append(fake_count)\n",
        "    return p"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Wcip-QWOBjP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## transform.py\n",
        "\"\"\"\n",
        "Author: Andreas RÃ¶ssler\n",
        "\"\"\"\n",
        "from torchvision import transforms\n",
        "\n",
        "xception_default_data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize((299, 299)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5]*3, [0.5]*3)\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize((299, 299)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5] * 3, [0.5] * 3)\n",
        "    ]),\n",
        "    'test': transforms.Compose([\n",
        "        transforms.Resize((299, 299)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5] * 3, [0.5] * 3)\n",
        "    ]),\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_YRQBfX7pKQ",
        "colab_type": "code",
        "outputId": "d7472173-3d87-4d8a-ed63-14ab99307e01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd /content/gdrive/My\\ Drive/FaceForensics-master/classification"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/FaceForensics-master/classification\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrMuoi32PDE4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#metadata = pd.read_json('/content/gdrive/My Drive/deepfake-detection-challenge/train_sample_videos/metadata.json').T\n",
        "import os\n",
        "def predict_model(video_fn, model,\n",
        "                  start_frame=0,plot_every_x_frames = 1):\n",
        "    \"\"\"\n",
        "    Given a video and model, starting frame and end frame.\n",
        "    Predict on all frames.\n",
        "    \n",
        "    \"\"\"\n",
        "    fn = video_fn.split('.')[0]\n",
        "    #label = metadata.loc[video_fn]['label']\n",
        "    #original = metadata.loc[video_fn]['original']\n",
        "    video_path = f'/content/gdrive/My Drive/videos/{video_fn}'\n",
        "    output_path = '/content/gdrive/My Drive/test'\n",
        "    p=test_full_image_network(video_path, model, output_path, start_frame=0, end_frame=200,cuda=False)\n",
        "    # Read output\n",
        "    fake_count=p[1]\n",
        "    end_frame=p[0]\n",
        "    vidcap = cv2.VideoCapture(f'{fn}.avi')\n",
        "    success,image = vidcap.read()\n",
        "    count = 0\n",
        "    #fig, axes = plt.subplots(150, 2, figsize=(20,15))\n",
        "    #axes = axes = axes.flatten()\n",
        "    path='/content/gdrive/My Drive/images'\n",
        "    os.chdir(path)\n",
        "    i = 0\n",
        "    #while success:\n",
        "        # Show every xth frame\n",
        "        #if count % plot_every_x_frames == 0:\n",
        "            #filen=str(count)+'.jpg'\n",
        "            #axes[i].imshow(image)\n",
        "            #cv2.imwrite(filen, image)\n",
        "            #image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "            #axes[i].set_title(f'{fn} - frame {count} - true label: {label}')\n",
        "            #axes[i].xaxis.set_visible(False)\n",
        "            #axes[i].yaxis.set_visible(False)\n",
        "            #i += 1\n",
        "        #success,image = vidcap.read()\n",
        "        #count += 1\n",
        "    #plt.tight_layout()\n",
        "    #plt.show()\n",
        "    print(fake_count)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RmbKY7cS_z-",
        "colab_type": "code",
        "outputId": "e44f5d86-ad18-411f-841f-6eca46372a70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "model_path = '/content/gdrive/My Drive/deepfakemodelspackages/faceforensics_models/faceforensics++_models_subset/full/xception/full_raw.p'\n",
        "model = torch.load(model_path, map_location=torch.device('cpu'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:593: SourceChangeWarning: source code of class 'network.models.TransferModel' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:593: SourceChangeWarning: source code of class 'pretrainedmodels.models.xception.Xception' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:593: SourceChangeWarning: source code of class 'torch.nn.modules.conv.Conv2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:593: SourceChangeWarning: source code of class 'torch.nn.modules.batchnorm.BatchNorm2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:593: SourceChangeWarning: source code of class 'torch.nn.modules.activation.ReLU' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:593: SourceChangeWarning: source code of class 'pretrainedmodels.models.xception.Block' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:593: SourceChangeWarning: source code of class 'torch.nn.modules.container.Sequential' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:593: SourceChangeWarning: source code of class 'pretrainedmodels.models.xception.SeparableConv2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:593: SourceChangeWarning: source code of class 'torch.nn.modules.pooling.MaxPool2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:593: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVFe3XLPPhSo",
        "colab_type": "code",
        "outputId": "3608da0c-f0d0-410a-aafc-c0de8aebd4c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83,
          "referenced_widgets": [
            "adf3dc7090274f65914983ebf937b2b5",
            "60bcda75a70f4a70b133a4df3ddcae88",
            "6aa5d25dfc4f4e33ad13c6570d34a791",
            "c45a789a8cf744d09c837c72a72b858e",
            "ab9a1ab909a0404881ac1455a7df3dcb",
            "540ab093c4044e8ca6c361e60183d12b",
            "c633f7ac1192498b97d2cac2aee40041",
            "7b20b18154bd4545b51fce171930948e"
          ]
        }
      },
      "source": [
        "predict_model('wixbuuzygv.mp4', model)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "adf3dc7090274f65914983ebf937b2b5",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=200.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "155\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}